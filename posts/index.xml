<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Rex&#39;s Notes 技術筆記 </title>
    <link>https://tsunejui.github.io/posts/</link>
    <description>Recent content in Posts on Rex&#39;s Notes 技術筆記 </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 19 Nov 2023 03:12:13 +0800</lastBuildDate><atom:link href="https://tsunejui.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Container Note - Mount Namespace</title>
      <link>https://tsunejui.github.io/posts/container-mnt/</link>
      <pubDate>Sun, 19 Nov 2023 03:12:13 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/container-mnt/</guid>
      <description>使用環境 Rocky Linux 8.7
環境描述 維護 kubernetes 時，常常需要透過一些 CLI tool 維護 container，如果 image 本身不提供這些 tool，就會比較麻煩，這時候我們可以透過 kubectl cp 將我們需要為戶的 CLI tool 複製到 container 內，但在官方的說明內有指出，使用 kubectl cp 會需要容器內有安裝 tar 指令，否則會失敗:
https://kubernetes.io/docs/reference/kubectl/cheatsheet/
我以 simple-spring-boot-app 這個專案為例，他的 base image 是 google 的 distroless，在這個 image 內，除了 java 外，並沒有其他的 command，這時候執行 kubectl cp，就會發生錯誤:
simple-spring-boot-app: https://github.com/tsunejui/simple-spring-boot-app
distroless: https://github.com/GoogleContainerTools/distroless
這時候，我們可能需要到 node 上，透過 container 的管理工具所提供的方法 (cp 指令)，將檔案複製到 container 內，一些工具如下:
Docker: docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- Podman: podman cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- cri-tools: crictl cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- nerdctl: nerdctl cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- 如果 node 上 container 的管理工具並沒有提供 cp 的功能，或者沒有網路安裝 container 的管理工具時，我們也找到該 process (也就是 container) 所使用 mnt namespace，並查看 / 的 mount point，直接將 CLI tool 放置於指定的目錄即可。</description>
    </item>
    
    <item>
      <title>AWS Security Token Service (STS) Example</title>
      <link>https://tsunejui.github.io/posts/aws-security-token-service-example/</link>
      <pubDate>Sun, 12 Nov 2023 21:33:35 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/aws-security-token-service-example/</guid>
      <description>環境描述 最近公司需要評估 AWS ROSA 在客戶端導入的可行性，因此需要研究其行為，根據 ROSA 的文件，建立 ROSA 的方式可以分成 with STS 及 without STS，由於之前沒有使用過 AWS STS ，此篇文章作為 STS API 的基本測試步驟：
基本介紹 根據 AWS 官方介紹，AWS STS API 提供了 trusted users 取得 Temporary security credentials 的方法。在 AWS ROSA 上可以帶來一些好處：
測試步驟 (請先設定好 ~/.aws/credentials) 建立 User Alice aws iam create-user --user-name Alice 建立 Example Policy 編輯 iam policy 檔案: vi example-policy.json
{ &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Sid&amp;#34;: &amp;#34;VisualEditor0&amp;#34;, &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Action&amp;#34;: [ &amp;#34;s3:ListStorageLensConfigurations&amp;#34;, &amp;#34;s3:ListAccessPointsForObjectLambda&amp;#34;, &amp;#34;s3:ListBucketMultipartUploads&amp;#34;, &amp;#34;s3:ListAllMyBuckets&amp;#34;, &amp;#34;s3:ListAccessPoints&amp;#34;, &amp;#34;s3:ListJobs&amp;#34;, &amp;#34;s3:ListBucketVersions&amp;#34;, &amp;#34;s3:ListBucket&amp;#34;, &amp;#34;s3:ListMultiRegionAccessPoints&amp;#34;, &amp;#34;s3:ListMultipartUploadParts&amp;#34; ], &amp;#34;Resource&amp;#34;: &amp;#34;*&amp;#34; } ] } 新增 iam policy:</description>
    </item>
    
    <item>
      <title>Scale Down the Prometheus on Openshift Monitoring</title>
      <link>https://tsunejui.github.io/posts/openshift-monitoring-prometheus-scale-down/</link>
      <pubDate>Tue, 07 Nov 2023 22:29:20 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/openshift-monitoring-prometheus-scale-down/</guid>
      <description>環境描述 客戶希望可以修改 Openshift 4.12 上 Prometheus 的 Replicas 數量為 0，希望我可以研究。
研究結果 目前 Openshift 4.12 沒有提供修改 Prometheus 數量的方法。 原本以為修改 Prometheus CRD 就可以更改 Replicas 的數量，但是修改後過沒多久就會被 Operator 改回來。以下是我找到的資料:
曾經在3.11可以調整replicas的設定，但後來消失了:
https://github.com/openshift/cluster-monitoring-operator/pull/330/commits/53453691c9d47f5c621a9d538aba12431541a2c8
我在 cluster-monitoring-operator 的設定上也沒找到參數可以調整，測試 replicas 也無法:
https://github.com/openshift/cluster-monitoring-operator/blob/master/Documentation/api.md#prometheusk8sconfig
目前我找到這個 issue，在 2020 年 8 月還不支援
https://github.com/openshift/cluster-monitoring-operator/issues/896
解決方法 現在這是我看過最暴力的做法，先透過修改 clusterversion/version ，將 openshift-monitoring 設定為 unmanaged，之後再 scale down 所有 deployment ，親測有效，但很暴力:
https://gist.github.com/waynedovey/cbf23d0a9c798c8de68b5f2043ba945b
oc patch clusterversion/version --type=&amp;#39;merge&amp;#39; -p &amp;#34;$(cat &amp;lt;&amp;lt;- EOF spec: overrides: - group: apps/v1 kind: Deployment name: cluster-monitoring-operator namespace: openshift-monitoring unmanaged: true EOF )&amp;#34; oc patch prometheus/k8s -n openshift-monitoring --type=&amp;#39;merge&amp;#39; -p &amp;#34;$(cat &amp;lt;&amp;lt;- EOF spec: replicas: 0 EOF )&amp;#34; oc patch alertmanagers/main -n openshift-monitoring --type=&amp;#39;merge&amp;#39; -p &amp;#34;$(cat &amp;lt;&amp;lt;- EOF spec: replicas: 0 EOF )&amp;#34; oc scale --replicas=0 deploy/cluster-monitoring-operator -n openshift-monitoring oc scale --replicas=0 deployment.</description>
    </item>
    
    <item>
      <title>F5 VIP Setting</title>
      <link>https://tsunejui.github.io/posts/f5-vip-setting/</link>
      <pubDate>Thu, 02 Nov 2023 15:20:48 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/f5-vip-setting/</guid>
      <description>環境描述 客戶環境需要在 F5 設定 VIP，並分配流量到不同的主機，我需要在公司 Lab 上模擬，由於對 F5 的操作不熟悉，因此撰寫此篇文章作為紀錄 VIP 設定方法。
請注意，此篇文章是簡易的設定紀錄，我並非 F5 的專業人員，如果錯誤請見諒
F5 設定介面 到 Local Traffic / Pools / Pool List 頁面上新增 Pool:
設定 Name 及 Health Monitors 的方式 (下圖為新增後的編輯畫面):
到 Local Traffic / Virtual Servers / Virtual Servers List 頁面上新增 Virtual Server:
到 Properties 頁面上設定 General Properties 及憑證相關資料 (下圖為新增後的編輯畫面):
到 Resource 頁面上設定 Load Balancing (下圖為新增後的編輯畫面):
結果 設定完成後便立即生效，在 Local Traffic / Pools : Pool List / &amp;lt;pool-name&amp;gt; 上可以看到 Pool 內的 members 狀況:</description>
    </item>
    
    <item>
      <title>Dex Expiration Time on Openshift Gitops</title>
      <link>https://tsunejui.github.io/posts/openshift-gitops-dex-expiration-time/</link>
      <pubDate>Thu, 02 Nov 2023 00:13:03 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/openshift-gitops-dex-expiration-time/</guid>
      <description>ArgoCD 認證機制:
修改流程:
環境描述 近期由於客戶需要導入 ArgoCD 管理 CRDs，因此在客戶的 Openshift 4.x 上安裝 GitOps Operator。安裝後發現了一個問題，ArgoCD 的 Web 介面需要約 24 小時後才會自動登出，客戶希望調整登出時間。
從上圖可得知，ArgoCD 的 web 會再登入後產生一把 token 並放到 cookie 內，名為 argocd.token，我們可以透過 jwt.io 解析這把 token，查看 expiration date:
在沒有做任何設定的情況下，透過 Openshift 帳密使用 SSO 登入機制進入 ArgoCD 畫面，所產生的 token 過期時間應該是 24h 後，這是因為 ArgoCD 在 SSO 認證上是交給 Dex Server 控制，而 Dex Server 的預設過期時間是 24h：
可以從 ArgoCD 的 Security 文章查看 Dex Server 的說明:
尋找方法 ArgoCD Operator 會透過 Kind: ArgoCD 的 CRD ，名為 openshift-gitops，產生需要的 components，如 server、applicationSet、grafana&amp;hellip;:</description>
    </item>
    
    <item>
      <title>NodeClockNotSynchronising alert troubleshooting in Openshift 4</title>
      <link>https://tsunejui.github.io/posts/node-clock-not-synchronising/</link>
      <pubDate>Sat, 26 Aug 2023 01:51:06 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/node-clock-not-synchronising/</guid>
      <description>事件經過 某日客戶收到來自 OCP 的 critical 告警，名稱為 NodeClockNotSynchronising，請我們調查原因。
告警訊息 首先調查告警意義，至 openshoft-monitoring Project 下尋找名為 prometheus-k8s-rulefiles-0 的 ConfigMap，便可看到以下內容
https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeClockNotSynchronising.md
- alert: NodeClockNotSynchronising annotations: description: Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host. runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeClockNotSynchronising.md summary: Clock not synchronising. expr: | min_over_time(node_timex_sync_status{job=&amp;#34;node-exporter&amp;#34;}[5m]) == 0 and node_timex_maxerror_seconds{job=&amp;#34;node-exporter&amp;#34;} &amp;gt;= 16 for: 10m labels: severity: critical 根據官方文件，我們可以得知以下訊息:
The NodeClockNotSynchronising alert triggers when a node is affected by issues with the NTP server for that node.</description>
    </item>
    
    <item>
      <title>RHEL Ansible Automation Platform (AAP) Installation</title>
      <link>https://tsunejui.github.io/posts/rhel-automation-platform-install/</link>
      <pubDate>Thu, 24 Aug 2023 10:24:53 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/rhel-automation-platform-install/</guid>
      <description>環境準備 Linux 的版本為 Red Hat Enterprise Linux 9.1 (Plow):
下載 tar 檔案 下載 Red Hat Ansible Automation Platform (v. 2.4 for RHEL 9 for x86_64): 至 https://access.redhat.com/downloads/content/480 下載 Ansible Automation Platform 2.4 Setup Bundle
解壓縮 tar xvzf ansible-automation-platform-setup-&amp;lt;latest-version&amp;gt;.tar.gz 安裝 Red Hat Ansible Automation Platform subscription 取得token 至 https://access.redhat.com/management/api 產生 Token
在 Redhat 上使用 subscription-manager register 註冊機器 你也可以透過 subscription-manager register --username &amp;lt;username&amp;gt; --password &amp;lt;password&amp;gt; --auto-attach 註冊機器
subscription-manager register --token &amp;lt;token&amp;gt; --insecure 獲得 Subscription 的 pool_id subscription-manager list --available --all | grep &amp;#34;Ansible Automation Platform&amp;#34; -B 3 -A 6 安裝 Subscription subscription-manager attach --pool=2c94e43f88fcee5b0189010269427636 驗證是否成功 subscription-manager list --consumed 建立 Ansible Automation Platform Database 如果需要自建 AAP 的 Database 則需執行此步驟，不需要請略過。</description>
    </item>
    
    <item>
      <title>AWX Installation (Container)</title>
      <link>https://tsunejui.github.io/posts/awx-install/</link>
      <pubDate>Mon, 17 Jul 2023 13:36:31 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/awx-install/</guid>
      <description>利用 dokcer 安裝 AWX
使用環境 Rocky Linux 8.7
安裝步驟 關閉 SELinux 建議暫時關閉 SELinux 避免安裝中出錯:
setenforce 0 安裝 docker https://docs.docker.com/engine/install/centos/
安裝 yum-utils:
yum install -y yum-utils 加入 repository:
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 安裝 docker
yum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin 啟動 docker
systemctl start docker 下載 AWX 17.1.0 安裝 git:
dnf install -y git Starting in version 18.0, the AWX Operator is the preferred way to install AWX.
https://github.com/ansible/awx/blob/devel/INSTALL.md
git clone -b 17.</description>
    </item>
    
    <item>
      <title>Samba 安裝</title>
      <link>https://tsunejui.github.io/posts/samba-install/</link>
      <pubDate>Sat, 01 Jul 2023 17:23:23 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/samba-install/</guid>
      <description>使用環境 Rocky Linux 8.7
安裝步驟 透過 dnf 安裝 samba:
dnf install samba 設定 systemd 開機自動啟動:
systemctl enable smb.service 開啟防火牆
firewall-cmd --permanent --add-service=samba &amp;amp;&amp;amp; firewall-cmd --reload 設定 samba 新增Linux User: rex
useradd rex 新增 samba 帳號: rex
smbpasswd -a rex 新增目錄 /mnt/shared 並指派 owner 為 rex:
mkdir /mnt/shared &amp;amp;&amp;amp; chown rex -R /mnt/shared/ 編輯 vi /etc/samba/smb.conf, 並且加入以下內容:
設定參數請參考: https://www.samba.org/samba/docs/current/man-html/smb.conf.5.html
... [cnssmb] comment = CNS Samba Directory path = /mnt/shared public = yes create mask = 0777 directory mask = 0777 browseable = yes writeable = yes read only = no 設定 SELinux:</description>
    </item>
    
    <item>
      <title>Kafka 安裝 - Kraft Mode</title>
      <link>https://tsunejui.github.io/posts/kafka-kraft-install/</link>
      <pubDate>Wed, 28 Jun 2023 10:43:24 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/kafka-kraft-install/</guid>
      <description>事前準備 Preparation 準備 3 台 node:
os name ip Rocky Linux 8.8 cns-kafka-0 192.168.50.170 Rocky Linux 8.8 cns-kafka-1 192.168.50.171 Rocky Linux 8.8 cns-kafka-2 192.168.50.172 安裝步驟 設定 hosts (在每個 Node 上執行) 透過以下指令設定 /etc/hosts:
cat &amp;lt;&amp;lt; EOF &amp;gt;&amp;gt; /etc/hosts 192.168.50.170 cns-kafka-0 192.168.50.171 cns-kafka-1 192.168.50.172 cns-kafka-2 EOF 安裝 OpenJDK 11 (在每個 Node 上執行) To use Corretto RPM repositories with the yum package manager (such as Amazon Linux AMI), import the Corretto public key and then add the repository to the system list.</description>
    </item>
    
    <item>
      <title>Node Exporter</title>
      <link>https://tsunejui.github.io/posts/node-exporter/</link>
      <pubDate>Fri, 16 Jun 2023 09:42:09 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/node-exporter/</guid>
      <description>安裝 node exporter 監控 linux
使用環境 Rocky Linux 8.7
操作步驟 下載 binary 檔案 下載 node-exporter
curl -OL https://github.com/prometheus/node_exporter/releases/download/v1.6.0/node_exporter-1.6.0.linux-amd64.tar.gz 解壓縮 解壓縮 tar 檔案
tar -xzf node_exporter-1.6.0.linux-amd64.tar.gz 設定 Binary 位置及權限 新增資料夾
mkdir /etc/node_exporter 搬移檔案
mv node_exporter-1.6.0.linux-amd64/node_exporter /etc/node_exporter/node_exporter 賦予權限:
chmod +x /etc/node_exporter/node_exporter 建立 Service 檔案 vi node_exporter.service 填入以下內容
[Unit] Description=Node Exporter Wants=network-online.target After=network-online.target [Service] Type=simple ExecStart=/bin/bash -c &amp;#39;/etc/node_exporter/node_exporter&amp;#39; [Install] WantedBy=multi-user.target 搬移到 /etc/systemd/system/
cp node_exporter.service /etc/systemd/system/ systemd 設定 重啟 daemon
systemctl daemon-reload 設定開機自動啟動
systemctl enable node_exporter.</description>
    </item>
    
    <item>
      <title>SSH Tunneling</title>
      <link>https://tsunejui.github.io/posts/ssh-tunnel/</link>
      <pubDate>Sat, 20 May 2023 12:34:03 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/ssh-tunnel/</guid>
      <description>What is SSH Tunneling SSH Tunneling is a method of creating an encrypted SSH connection between a client and a server machine through which services ports can be relayed, for example:
Pre-requisites Rocky Linux VM x 2 Install Grafana on one of them vm ip service port VM I (SSH Client) 10.250.75.103 VM II (SSH Server) 10.250.75.147 3000 (Grafana) 確保 /etc/ssh/sshd_config 內的 AllowTcpForwarding 為 yes Get Started 使用 ssh command 在 VM I:</description>
    </item>
    
    <item>
      <title>RHEL 9 在 Proxmox 上 Kernel Panic</title>
      <link>https://tsunejui.github.io/posts/rhel-9-panic/</link>
      <pubDate>Tue, 16 May 2023 09:43:12 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/rhel-9-panic/</guid>
      <description>環境描述 使用 Proxmox 7.2-3:
問題原因 在 Proxmox 上安裝 RHEL 9 時會發生 Kernel Panic:
解決方法 在新增 VM 時，CPU type 選擇 host:
成功啟動 RHEL 9:
Reference https://access.redhat.com/discussions/6959360 </description>
    </item>
    
    <item>
      <title>NFS on Kubernetes</title>
      <link>https://tsunejui.github.io/posts/nfs-k8s/</link>
      <pubDate>Sun, 14 May 2023 23:32:13 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/nfs-k8s/</guid>
      <description>環境設定 項目 內容 NFS Server 10.250.75.111 NFS Path /mnt/nfs_shares 部署 NFS Storage Class 以下步驟將建立 Custom Provisioner，並串接 NFS Server。
建立 Provisioner 的 Service Account 請參考 here
建立名為 nfs-client-provisioner 的 Service Account
kubectl apply -f account.yaml 建立 Provisioner Deployment 請參考 here
kubernetes-sigs/nfs-subdir-external-provisioner: https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner
為了避免出錯，strategy.type 設定為 Recreate，而 replicas 設為 1
使用 kubernetes-sigs/nfs-subdir-external-provisioner 作為 provisioner，請注意以下環境變數設定:
- name: PROVISIONER_NAME # storage class 名稱 value: k8s-sigs.io/nfs-subdir-external-provisioner - name: NFS_SERVER # nfs server 位置 value: 10.250.75.111 - name: NFS_PATH # nfs directory 路徑 value: /mnt/nfs_shares 使用 kubectl 部署 nfs-client-provisioner:</description>
    </item>
    
    <item>
      <title>NFS on Rocky Linux 8</title>
      <link>https://tsunejui.github.io/posts/nfs/</link>
      <pubDate>Sat, 13 May 2023 17:36:21 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/nfs/</guid>
      <description>什麼是 NFS NFS (Network File System) 是一種分散式檔案系統協議，用於在網路上共享檔案和資源。
環境設定 id name ip 1 nfs-server 10.250.75.111 2 nfs-client 10.250.75.103 在 NFS Server 上 安裝 nfs 工具包 dnf -y install nfs-utils 啟動 nfs server 並且設定開機自動開啟。 systemctl enable --now nfs-server.service 建立 nfs 目錄 mkdir -p /mnt/nfs_shares 更改 owner 及設定權限 nobody 使用者是一個系統使用者，不擁有任何特殊權限，且通常用於運行低權限的服務或進程。
chown -R nobody:nobody /mnt/nfs_shares &amp;amp;&amp;amp; chmod -R 777 /mnt/nfs_shares vim /etc/exports /etc/exports 是在 Linux 系統中用於配置 NFS 的設定檔案。NFS 是一種分散式檔案系統，它允許不同的機器透過網路共享檔案。
rw: 可讀可寫
sync: 寫入後同步
no_all_squash: 禁用 all_squash</description>
    </item>
    
    <item>
      <title>Proxmox 增加 Linux 硬碟空間</title>
      <link>https://tsunejui.github.io/posts/proxmox-extend-linux-space/</link>
      <pubDate>Tue, 02 May 2023 02:02:22 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/proxmox-extend-linux-space/</guid>
      <description>當 Proxmox 上的 Linux 硬碟可用空間不足時, 可以透過以下方法增加空間。
使用環境 Rocky Linux 8.7
Proxmox 重新設定硬碟空間 進入 Proxmox 上 VM 的 Hardware, 編輯 Hard Disk:
增加硬碟空間:
透過 lsblk 可以看到, 硬碟 sda 容量有增加。
lsblk 調整 LVM 我們要調整 rl-root 的容量, 也就是 LV 的大小, 而調整 LV 需要先建立 PV, 再將 PV 加入與 LV 相同的 VG, 如下圖所示:
https://networklessons.com/uncategorized/extend-lvm-partition
在建立 PV 之前, 我們需要建立 partition, 透過 parted 指令可以輕鬆的達成:
如果要操作的硬碟不是 /dev/sda, 你需要下 select /dev/sdb, 請將 /dev/sdb 改成你希望操作的硬碟
parted 進入 parted 畫面後, 查看可用空間, 及 Start 和 End:</description>
    </item>
    
    <item>
      <title>Rocky Linux 使用 XRDP</title>
      <link>https://tsunejui.github.io/posts/rocky-xrdp/</link>
      <pubDate>Sun, 30 Apr 2023 20:53:09 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/rocky-xrdp/</guid>
      <description>使用環境 Rocky Linux 8.7
安裝 xrdp server 安裝 xrdp:
dnf install xrdp 啟動 server 啟動 xrdp server:
systemctl start xrdp 設定開機啟動:
systemctl enable xrdp 打開防火牆 預設 port 為 3389:
firewall-cmd --permanent --add-port=3389/tcp &amp;amp;&amp;amp; firewall-cmd --reload 測試 使用 Microsoft Remote Desktop 測試連線:
連線成功:
Reference https://linux.how2shout.com/how-to-connect-rocky-linux-8-via-windows-rdp-protocol/ </description>
    </item>
    
    <item>
      <title>Install GUI on Rocky Linux 8.7</title>
      <link>https://tsunejui.github.io/posts/rocky-install-gui/</link>
      <pubDate>Sun, 30 Apr 2023 20:14:50 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/rocky-install-gui/</guid>
      <description>使用環境 Rocky Linux 8.7
YUN packages 更新 更新安裝包 Update Your Server
yum update 檢查 Package Groups 我們需要使用 Server with GUI, 請使用 yum group list檢查是否在列表內:
yum group list 安裝 Server with GUI 執行下面的指令:
yum groupinstall &amp;#34;Server with GUI&amp;#34; 設定開機進入圖形介面 更改 /etc/systemd/system/default.target 指向的位置：
systemctl set-default graphical 重開機 reboot 結果 成功啟動圖形介面:
Reference https://www.cyberithub.com/how-to-install-gnome-desktop-gui-on-rocky-linux-8/ </description>
    </item>
    
    <item>
      <title>Kafka 學習筆記</title>
      <link>https://tsunejui.github.io/posts/kafka/</link>
      <pubDate>Sun, 12 Mar 2023 17:52:27 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/kafka/</guid>
      <description>Kafka Installation Kafka official site: https://kafka.apache.org/downloads
Kafka provides two types of downloads on its official website, namely Source downloads and Binary downloads. The difference between the two is that Source download means Kafka&amp;rsquo;s source code, and users need to compile it themselves after downloading, while Binary Download means pre-compiled binaries that users can download and use directly. Additionally, different versions of Kafka written in different Scala versions are available for download in Binary Download.</description>
    </item>
    
    <item>
      <title>Istio Tip</title>
      <link>https://tsunejui.github.io/posts/istio-tip/</link>
      <pubDate>Wed, 08 Mar 2023 12:52:25 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/istio-tip/</guid>
      <description>istio Init:CrashLoopBackOff 問題 安裝 istio 後會發現所有 POD 顯示 Init:CrashLoopBackOff, 透過查看 log 可以發現錯誤。
https://github.com/istio/istio/issues/23009
需要載入以下 mod 修復問題：
modprobe br_netfilter ; modprobe nf_nat ; modprobe xt_REDIRECT ; modprobe xt_owner; modprobe iptable_nat; modprobe iptable_mangle; modprobe iptable_filter cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/99-istio-modules.conf br_netfilter nf_nat xt_REDIRECT xt_owner iptable_nat iptable_mangle iptable_filter EOF </description>
    </item>
    
    <item>
      <title>Kubernetes Tips</title>
      <link>https://tsunejui.github.io/posts/kubernetes-tips/</link>
      <pubDate>Wed, 08 Mar 2023 12:50:23 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/kubernetes-tips/</guid>
      <description>產生新的加入 cluster 指令 建立新的 token
kubeadm token generate 畫面會顯示新的 token, 便可以使用 kubeadm token create 建立新的 join command
以 4a4dv5.i31huo1je1e2dler 為例
kubeadm token create 4a4dv5.i31huo1je1e2dler --print-join-command </description>
    </item>
    
    <item>
      <title>Kubernetes 安裝筆記</title>
      <link>https://tsunejui.github.io/posts/kubernetes-install/</link>
      <pubDate>Wed, 08 Mar 2023 10:19:24 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/kubernetes-install/</guid>
      <description>安裝 Kubernetes 事前準備 Preparation Before you begin
Rocky Linux ISO 載點: link
Kubernetes 安裝建議高於 2 core CPU 及 2 GB or more RAM: link
準備 5 台 node, OS 選擇 Rocky Linux 8 每台 node 需要 4 core CPU &amp;amp; 8 GB memory (非必要) 6 組 IP (node * 5 + VIP * 1) Kubernetes Interface
interface service version CRI containerd 1.6.19 CNI calico 3.25.0 nodes
建議登入每台主機, 並使用 hostnamectl set-hostname xxx 修改 hostname</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>https://tsunejui.github.io/posts/aboutme/</link>
      <pubDate>Tue, 07 Mar 2023 15:17:43 +0800</pubDate>
      
      <guid>https://tsunejui.github.io/posts/aboutme/</guid>
      <description>Hi, welcome to my first Blog Post. YAY!
I&amp;rsquo;m so ready to have a place to share all my coding travels on, and share my life. Let me introduce myself first:
I&amp;rsquo;m Rex, a software engineer. I am passionate about technology and keep continually focusing the solutions of SRE、DevOps and Data Visualization. I am looking forward to applying my skill set into any challenge in the work. contact me if you need assistance!</description>
    </item>
    
  </channel>
</rss>
